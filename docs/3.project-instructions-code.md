# Project Instructions

Now, you should be all set up and ready to get started on your People Counter app using the OpenVINO™ Toolkit. Although you are free to dive right into the starter code and work everything out on your own, we’ve also provided some additional helpful instructions below to help you along the way. First, we’ll cover the steps to get your code up and running, and then we’ll get into what to do in the write-up portion of the project.



## Project Instructions: Code

As noted in the project introduction, there are two primary files in which you’ll work to implement the People Counter app - `inference.py` and `main.py`. The `inference.py` file is the suggested starting point, as you’ll need much of the code there to fully implement the `TODO`’s throughout `main.py`.

To start, you’ll need to choose a model and utilize the Model Optimizer to begin.



#### Choosing a Model & The Model Optimizer

You will choose the model you want to work with in this project - the only requirement is that the model is not already one of the pre-converted Intermediate Representations already available from Intel®.

Please provide a link for your reviewer to the original model you choose, as well as the what you entered into the command line to convert your model with the Model Optimizer. You only need to include the converted IR model in your submission, not the original.

While what type of model you use is up to you to decide, you must be able to extract information regarding the number of people in the frame, how long they have been in frame, and the total amount of people counted from your model’s output. Models using bounding boxes or semantic masks as the output will likely be helpful in that regard, but feel free to use any model you think might be helpful here.

Note that you may need to do additional processing of the output to handle incorrect detections, such as adjusting confidence threshold or accounting for 1-2 frames where the model fails to see a person already counted and would otherwise double count.

**If you are otherwise unable to find a suitable model after attempting and successfully converting at least three other models**, you can document in your write-up what the models were, how you converted them, and why they failed, and then utilize any of the Intel® Pre-Trained Models that may perform better.



#### Inference.py

1. Start by taking a look through the starter code to familiarize yourself with it. In the file, you will implement parts of the `Network` class, which will be used in `main.py` to interact with the loaded, pre-trained model.
   - You’ll actually be able to use code quite similar to what you implemented in the final exercise of the course, but using a new model and different information sent over MQTT.
   - **You do not have to exactly use the starter template.** If you want to change up the functions within, feel free to do so.

2. In `load_model()`  :
   - Set a `self.plugin` variable with `IECore`
   - If applicable, add a CPU extension to `self.plugin`
   - Load the Intermediate Representation model
   - Check whether all layers are supported, or use a CPU extension. If a given layer is not supported, let the user know which layers
   - Load the model network into a `self.net_plugin` variable

3. Implement `get_input_shape()` to return the shape of the input layer

4. Implement `exec_net()` by setting a `self.infer_request_handle` variable to an instance of `self.net_plugin.start_async`

5. Implement `get_output()` by handling how results are returned based on whether an `output` is provided to the function.



#### Main.py

1. Implement `connect_mqtt()` by creating and connecting to the MQTT client
2. In `infer_on_stream()` load the model into a `infer_network` variable
3. Implement handling video or image input, or note to the user that it was unable to use the input
4. Use CV2 to read from the video capture
5. Pre-process the image frame as needed for input into the network model
6. Implement processing of the network output
   - Extract any bounding boxes, semantic masks, etc. from the result - make sure to use `prob_threshold` if working with bounding boxes!
   - If using bounding boxes, draw the resulting boxes with `cv2.rectangle` on the frame
   - Update `current_count` as necessary
7. Back in `infer_on_stream()`, calculate and send relevant information on `count`, `total` and `duration` to the MQTT server
8. Send the frame (now including any relevant output information) to the FFmpeg server
9. Separately, if the user input a single image, write out an output image.

It’s been a lot of hard work, but now it’s time to check out your app, deployed at the edge!



https://classroom.udacity.com/nanodegrees/nd131/parts/d334a449-003a-48fb-8d8a-079cba821e76/modules/efc7c11f-29c0-43d5-a7a9-1a7c25fe0c4b/lessons/ff92f8be-2afd-4077-90f9-f6ddc1624e9f/concepts/2ff97d34-1e14-4522-b400-b3bfa4eb2feb