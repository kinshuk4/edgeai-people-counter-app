# Running the App

### Sourcing the Environment

When opening a new terminal window, in order to source the environment, use the command:

```bash
source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5
```

Any new terminals you open will again need this command run.

### Starting the Servers

Before running `main.py`, you will need to have the MQTT, FFmpeg and UI servers all up and running. If you are running your app on a local device, make sure you have followed the earlier set-up instructions so that all dependencies are installed.

You’ll need terminals open for each of these. For each, `cd` into the main directory containing all of your People Counter app files.

**Note**: You will need to run `npm install` in the `webservice/server` and `webservice/ui` directories if you have not already.

From there:

- For the MQTT server:

  ```
  cd webservice/server/node-server
  node ./server.js
  ```

- For the UI:

  ```
  cd webservice/ui
  npm run dev
  ```

- For the FFPMEG server:

  ```
  sudo ffserver -f ./ffmpeg/server.conf
  ```



### Starting the App Itself

As you may have noticed, there are a number of arguments that can be passed into `main.py` when run from a terminal. While you should make sure to check them out in the code itself, it’s important to note you’ll also want to add some additional commands to make sure the output image frames are sent to the FFmpeg server.

The arguments for `main.py` can be entered as follows (you may need to specify `python3` on your system):

```
python main.py -i {path_to_input_file} -m {path_to_model} -l {path_to_cpu_extension} -d {device_type} -pt {probability_threshold}
```

The arguments for FFmpeg can be entered as follows - note that we’ll include the values here that will work optimally with the FFmpeg server and UI instead of placeholders. If you have not updated the FFmpeg `server.conf` file, this will match to what is configured therein.

```
ffmpeg -v warning -f rawvideo -pixel_format bgr24 -video_size 768x432 -framerate 24 -i - http://0.0.0.0:3004/fac.ffm
```

To run these two together, *while you have the ffserver, MQTT server, and UI server already running through separate terminals*, you can use a pipe symbol (`|`) to combine them as one. Here is an example, along with possible paths for `main.py` arguments included:

```
python main.py -i resources/Pedestrian_Detect_2_1_1.mp4 -m your-model.xml -l /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so -d CPU -pt 0.6 | ffmpeg -v warning -f rawvideo -pixel_format bgr24 -video_size 768x432 -framerate 24 -i - http://0.0.0.0:3004/fac.ffm
```

*Note*: The primary CPU extension file differs in naming between Linux and Mac. On Linux, the name is `libcpu_extension_sse4.so`, while on Mac it is `libcpu_extension.dylib`.



### Viewing the App in Your Browser

If you are in the classroom workspace, use the “Open App” button to view the app in the browser, or if working locally, navigate to [http://0.0.0.0:3004](http://0.0.0.0:3004/) in your browser. You should be able to see the video stream with any relevant outputs (bounding boxes, semantic masks, etc.) onto the video. Additionally, you can click the icon in the upper right to expand to show some statistical information; clicking another icon under the existing charts on this new menu will expand the final piece of information.



https://classroom.udacity.com/nanodegrees/nd131/parts/d334a449-003a-48fb-8d8a-079cba821e76/modules/efc7c11f-29c0-43d5-a7a9-1a7c25fe0c4b/lessons/ff92f8be-2afd-4077-90f9-f6ddc1624e9f/concepts/d13b9c95-154f-463f-9336-33eebb3b6882